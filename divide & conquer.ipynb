{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe9a9d3-1632-43da-b77d-f43716ef2a28",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ac8225-3ea9-4f2d-ac3b-8dc3df0eb393",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.3823591e-01,  6.1981279e-01, -1.9406676e-01,  4.1382647e-01,\n",
       "        6.7421144e-01,  1.4040026e-01,  1.6097938e+00, -6.3083977e-01,\n",
       "       -1.0239607e+00, -5.4499447e-02, -9.9497877e-02, -3.1477413e-01,\n",
       "        1.7324188e-01,  1.8793076e-01,  7.9062946e-02,  5.0363833e-01,\n",
       "        1.4413389e+00,  1.2055126e-01,  8.0126077e-02,  2.2117047e-01,\n",
       "       -3.6095098e-01, -5.7067740e-01,  2.4038902e-01, -9.4271898e-01,\n",
       "        5.5764532e-01, -2.8166807e-01,  4.3514800e-01,  3.5518160e-01,\n",
       "       -2.5896186e-01,  1.4379877e-01, -5.8390427e-01,  8.1498146e-01,\n",
       "        2.8396347e-01,  2.5972643e-01,  3.5667416e-01, -2.3208426e-01,\n",
       "       -1.1156581e-02,  2.1004668e-01, -3.4496439e-01,  7.1494210e-01,\n",
       "        6.2013745e-01, -1.5306453e+00,  1.2590113e-01,  4.3290872e-02,\n",
       "       -3.9522484e-01,  1.8227801e+00,  8.5033786e-01, -1.2005626e+00,\n",
       "       -7.1115762e-01,  1.8970840e-01,  5.3540075e-01,  1.1116323e+00,\n",
       "       -5.1466793e-01,  5.6390721e-01, -1.5165229e+00, -9.0429567e-02,\n",
       "        1.4358957e-01, -3.8269025e-01, -1.5105064e+00, -9.2643863e-01,\n",
       "       -8.5899025e-01,  4.8373476e-01, -3.3635952e-02, -7.3084569e-01,\n",
       "       -6.3448554e-01,  1.8308429e-02,  9.4611782e-01, -7.5829804e-01,\n",
       "       -2.3259158e-01, -7.2087711e-01,  5.7928681e-01, -2.3938294e-01,\n",
       "       -2.2777092e-01, -6.7970890e-01,  4.4408053e-01, -4.6670315e-01,\n",
       "       -4.1924629e-01,  6.4906335e-01, -4.2146298e-01,  1.8714193e-01,\n",
       "        1.0497658e+00, -1.9623742e-01,  2.3333059e-01,  2.2360820e-01,\n",
       "        8.3412647e-01, -2.4868779e-01,  3.6563972e-01,  3.4461522e-01,\n",
       "        9.5194888e-01,  5.9169716e-01, -4.5143899e-01, -1.6296060e-01,\n",
       "       -1.7266154e-03, -4.6493882e-01, -7.2363597e-01,  6.1634239e-02,\n",
       "        1.1612383e+00,  1.8179925e-01, -7.6578736e-02, -2.8469682e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bnlp import BengaliWord2Vec\n",
    "import numpy as np\n",
    "\n",
    "bwv = BengaliWord2Vec()\n",
    "model_path = \"bangla_word2vec/bnwiki_word2vec.model\"\n",
    "\n",
    "\n",
    "def vectorize_word(word):\n",
    "    return bwv.generate_word_vector(model_path, word)\n",
    "vectorize_word(\"নৃগোষ্ঠী\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6a78249-4917-43eb-ae4c-93b2f7a8efbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "পর\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def preprocess_word(word): ## hANDLE DARI ।\n",
    "  translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
    "  word =  word.translate(translate_table)\n",
    "  return word.replace(\"।\", \"\")\n",
    "\n",
    "print(preprocess_word(\"পর,\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a41774-c614-4ad4-88e8-969d5b63b8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "পর\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def preprocess_word(word): ## hANDLE DARI ।\n",
    "  translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
    "  word =  word.translate(translate_table)\n",
    "  return word.replace(\"।\", \"\")\n",
    "\n",
    "print(preprocess_word(\"পর,\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faefbea1-ba58-48cc-928a-92bafcec8375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDatasetFromIBOFile(filename): #nlp_hackathon_bd_2023/data/dev.txt\"\n",
    "    f = open(filename, \"r\")\n",
    "    content = f.read()\n",
    "    \n",
    "    lines = content.split(\"\\n\")\n",
    "    \n",
    "    dataset = []\n",
    "    for line in lines:\n",
    "        parts = line.split(\" _ _ \")\n",
    "        word = parts[0]\n",
    "        word = preprocess_word(word)\n",
    "        label = parts[len(parts)-1]\n",
    "\n",
    "        dataset.append([word,label])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de4d5161-5b82-4646-96e5-0204c1756252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encodeClass(Y):\n",
    "    classes = [0,0] # Yes, No(Not NER)\n",
    "    \n",
    "    new_y = []\n",
    "    for cls in Y:\n",
    "        if cls=='O': new_y.append(0)\n",
    "        else: new_y.append(1)\n",
    "    return new_y\n",
    "                \n",
    "# encodeClass(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7cf29a5-720a-406d-af6d-c9a3a1d6d2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for row in dataset:\n",
    "      word = row[0]\n",
    "      label = row[1]\n",
    "\n",
    "      print(word)\n",
    "\n",
    "      X.append(vectorize_word(word))\n",
    "      Y.append(label)\n",
    "    \n",
    "    Y = encodeClass(Y)\n",
    "    \n",
    "    Y = np.array(Y)\n",
    "    X = np.array(X)\n",
    "    return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "148f61e2-8814-4c74-b745-7cc500cf59be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['তিনি', 'O'], ['যুবক', 'O'], ['হিসেবে', 'O'], ['শেফিল্ড', 'B-GRP'], ['বুধবার', 'I-GRP'], ['এফসি', 'I-GRP'], ['যোগদান', 'O'], ['করেন', 'O'], ['এবং', 'O'], ['১৯৫১', 'O'], ['সালে', 'O']]\n",
      "[0 0 0 1 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae059dfc-9bbc-4064-840e-c927d512f900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "তিনি\n",
      "যুবক\n",
      "হিসেবে\n",
      "শেফিল্ড\n",
      "বুধবার\n",
      "এফসি\n",
      "যোগদান\n",
      "করেন\n",
      "এবং\n",
      "১৯৫১\n",
      "সালে\n"
     ]
    }
   ],
   "source": [
    "DATA_SIZE = 11\n",
    "dataset = getDatasetFromIBOFile(\"nlp_hackathon_bd_2023/data/dev.txt\")[:DATA_SIZE]\n",
    "X, Y = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c929080-eec7-44f8-96c9-625ce8bc3ded",
   "metadata": {},
   "source": [
    "### Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58089bd4-7661-4017-b4de-4e7dcca06ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed5ba890-91ff-4b36-a7a4-80598839fbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 59,010\n",
      "Trainable params: 59,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "binary_model = Sequential([\n",
    "    layers.Dense(256, input_dim=100, activation=\"relu\", name=\"input\"),\n",
    "    layers.Dense(128, activation=\"relu\", name=\"layer1\"),\n",
    "    #layers.Dense(64, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(2, activation=\"softmax\", name=\"output\")\n",
    "])\n",
    "binary_model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "binary_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "75926cce-08e6-4509-b98f-5280ac2e1472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples, validate on 2 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 5 which is outside the valid range of [0, 2).  Label values: 3 4 0 1 5 1 4 2 2\n\t [[{{node loss_10/output_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-06b24fc0d88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbinary_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 5 which is outside the valid range of [0, 2).  Label values: 3 4 0 1 5 1 4 2 2\n\t [[{{node loss_10/output_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]"
     ]
    }
   ],
   "source": [
    "binary_model.fit(x=X, y=Y, batch_size=10, epochs=30, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3340c039-d13a-467a-8bf6-008a605738c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8984386 , 0.10156144],\n",
       "       [0.90100056, 0.09899945],\n",
       "       [0.9836235 , 0.01637647],\n",
       "       [0.10870043, 0.8912995 ],\n",
       "       [0.21083497, 0.789165  ],\n",
       "       [0.15912646, 0.84087354],\n",
       "       [0.9734622 , 0.02653786],\n",
       "       [0.9865841 , 0.01341592],\n",
       "       [0.9514842 , 0.04851579],\n",
       "       [0.7769665 , 0.2230335 ],\n",
       "       [0.62828356, 0.3717165 ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049bea29-2f7b-4e4d-afb5-66b0f1f045dd",
   "metadata": {},
   "source": [
    "# Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44c146ea-f6af-4a12-9199-b5fc263da59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNERDatasetFromIBOFile(filename): #nlp_hackathon_bd_2023/data/dev.txt\"\n",
    "    f = open(filename, \"r\")\n",
    "    content = f.read()\n",
    "    \n",
    "    lines = content.split(\"\\n\")\n",
    "    \n",
    "    dataset = []\n",
    "    for line in lines:\n",
    "        parts = line.split(\" _ _ \")\n",
    "        word = parts[0]\n",
    "        word = preprocess_word(word)\n",
    "        label = parts[len(parts)-1]\n",
    "\n",
    "        if label!='O' and word!='': dataset.append([word,label])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ee6c31ba-44b9-4ff1-a40c-acf91979e76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_classes = [] # Global\n",
    "def encodeMultiClass(Y):\n",
    "    for val in Y:\n",
    "        if val not in global_classes:\n",
    "            global_classes.append(val)\n",
    "            \n",
    "    new_y = []\n",
    "    for val in Y:\n",
    "        for idx, cls in enumerate(global_classes):\n",
    "            if cls==val: \n",
    "                new_y.append(idx)\n",
    "                break\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc77cf9f-9ecb-4511-9ae4-eef4db8d4ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for row in dataset:\n",
    "      word = row[0]\n",
    "      label = row[1]\n",
    "\n",
    "      print(word)\n",
    "\n",
    "      X.append(vectorize_word(word))\n",
    "      Y.append(label)\n",
    "    \n",
    "    Y = encodeMultiClass(Y)\n",
    "    \n",
    "    Y = np.array(Y)\n",
    "    X = np.array(X)\n",
    "    return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7cc26382-3969-41e6-bed5-24e2eef7c925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "শেফিল্ড\n",
      "বুধবার\n",
      "এফসি\n",
      "চাটনি\n",
      "পোলিশ\n",
      "লিথুয়ানিয়ান\n",
      "কমনওয়েলথ\n",
      "আলোকচিত্রগ্রাহী\n",
      "ফিল্ম\n",
      "ভ্যারাইটি\n",
      "জাপানের\n"
     ]
    }
   ],
   "source": [
    "DATA_SIZE = 11\n",
    "dataset = getNERDatasetFromIBOFile(\"nlp_hackathon_bd_2023/data/dev.txt\")[:DATA_SIZE]\n",
    "X, Y = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d2c075d-325a-45f5-8dde-36a4c93385fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2 3 4 4 2 5 6 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f04f9a50-b5a9-445c-beae-bf755b88f6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 59,655\n",
      "Trainable params: 59,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_CLASSES = 7\n",
    "multiclass_model = Sequential([\n",
    "    layers.Dense(256, input_dim=100, activation=\"relu\", name=\"input\"),\n",
    "    layers.Dense(128, activation=\"relu\", name=\"layer1\"),\n",
    "    #layers.Dense(64, activation=\"relu\", name=\"layer2\"),\n",
    "    layers.Dense(NUM_OF_CLASSES, activation=\"softmax\", name=\"output\")\n",
    "])\n",
    "multiclass_model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "multiclass_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e844e03-8f28-48d6-b287-de04f190c86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples, validate on 2 samples\n",
      "Epoch 1/30\n",
      "9/9 - 0s - loss: 2.1973 - acc: 0.1111 - val_loss: 1.7508 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "9/9 - 0s - loss: 2.1352 - acc: 0.2222 - val_loss: 1.7692 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "9/9 - 0s - loss: 2.0745 - acc: 0.2222 - val_loss: 1.7876 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "9/9 - 0s - loss: 2.0147 - acc: 0.2222 - val_loss: 1.8065 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "9/9 - 0s - loss: 1.9556 - acc: 0.2222 - val_loss: 1.8249 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "9/9 - 0s - loss: 1.8979 - acc: 0.2222 - val_loss: 1.8434 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "9/9 - 0s - loss: 1.8423 - acc: 0.2222 - val_loss: 1.8618 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "9/9 - 0s - loss: 1.7888 - acc: 0.2222 - val_loss: 1.8801 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "9/9 - 0s - loss: 1.7371 - acc: 0.3333 - val_loss: 1.8984 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "9/9 - 0s - loss: 1.6864 - acc: 0.3333 - val_loss: 1.9167 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "9/9 - 0s - loss: 1.6367 - acc: 0.3333 - val_loss: 1.9348 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "9/9 - 0s - loss: 1.5884 - acc: 0.3333 - val_loss: 1.9529 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "9/9 - 0s - loss: 1.5416 - acc: 0.3333 - val_loss: 1.9707 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "9/9 - 0s - loss: 1.4965 - acc: 0.5556 - val_loss: 1.9877 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "9/9 - 0s - loss: 1.4532 - acc: 0.5556 - val_loss: 2.0044 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "9/9 - 0s - loss: 1.4116 - acc: 0.5556 - val_loss: 2.0208 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "9/9 - 0s - loss: 1.3722 - acc: 0.5556 - val_loss: 2.0370 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "9/9 - 0s - loss: 1.3344 - acc: 0.6667 - val_loss: 2.0527 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "9/9 - 0s - loss: 1.2988 - acc: 0.6667 - val_loss: 2.0678 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "9/9 - 0s - loss: 1.2646 - acc: 0.6667 - val_loss: 2.0819 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "9/9 - 0s - loss: 1.2320 - acc: 0.6667 - val_loss: 2.0958 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "9/9 - 0s - loss: 1.2006 - acc: 0.6667 - val_loss: 2.1094 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "9/9 - 0s - loss: 1.1702 - acc: 0.7778 - val_loss: 2.1226 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "9/9 - 0s - loss: 1.1409 - acc: 0.8889 - val_loss: 2.1359 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "9/9 - 0s - loss: 1.1126 - acc: 0.8889 - val_loss: 2.1487 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "9/9 - 0s - loss: 1.0856 - acc: 0.8889 - val_loss: 2.1609 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "9/9 - 0s - loss: 1.0595 - acc: 0.8889 - val_loss: 2.1728 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "9/9 - 0s - loss: 1.0347 - acc: 0.8889 - val_loss: 2.1841 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "9/9 - 0s - loss: 1.0110 - acc: 0.8889 - val_loss: 2.1952 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "9/9 - 0s - loss: 0.9881 - acc: 0.8889 - val_loss: 2.2057 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f096fcadd10>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_model.fit(x=X, y=Y, batch_size=10, epochs=30, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de9533-d661-4ac7-8fef-5b5d4b495730",
   "metadata": {},
   "source": [
    "# Make prediction using these two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "42bb3897-6131-4d90-9e59-fa750aa1f750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictClass(word, binary_classifier, multiclass_classifier):\n",
    "    x = vectorize_word(word).reshape(1,100)\n",
    "    predictions = binary_classifier.predict(x)\n",
    "    print(predictions)\n",
    "    if predictions[0][0] > predictions[0][1]: return \"O\"\n",
    "\n",
    "    cls = list(multiclass_classifier.predict(x))\n",
    "\n",
    "    print(cls)\n",
    "    max_value = max(cls[0])\n",
    "    max_index = list(cls[0]).index(max_value)\n",
    "    print(max_value, max_index)\n",
    "\n",
    "    \n",
    "    return global_classes[max_index]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9578cb8f-13ee-4ec6-b5e9-7d701d6de63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13426256 0.86573744]]\n",
      "[array([0.22747843, 0.04951087, 0.10875148, 0.0873021 , 0.15978889,\n",
      "       0.02095048, 0.34621775], dtype=float32)]\n",
      "0.34621775 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'B-CORP'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictClass(\"কমনওয়েলথ\", binary_model, multiclass_model)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
